# Feature 0003: AI Chat Integration

## Description
Implement a context-aware AI chat system that answers questions about sewer inspection data using streaming responses. The system will integrate OpenAI's GPT-3.5-turbo model with Server-Sent Events (SSE) for real-time streaming responses, providing specific answers based on the actual inspection data.

## Files to Create

### lib/ai-context.ts
- **Purpose**: Data context functions for AI queries
- **Key Function**: `getDataContext(query: string): Promise<SewerInspection[]>`
- **Algorithm**:
  1. Analyze query for relevant keywords (city names, materials, scores, defects)
  2. Build SearchFilters based on detected parameters
  3. Use existing searchInspections function to get relevant data
  4. Limit context to 50-100 records to stay within token limits
  5. Return structured data for AI prompt context
- **Context Selection Strategy**:
  - Extract location names from query
  - Identify material types mentioned
  - Detect score ranges or repair status keywords
  - Prioritize recent inspections if no specific filters

### app/api/chat/route.ts
- **Purpose**: Streaming AI endpoint with Server-Sent Events
- **HTTP Method**: GET
- **Query Parameters**: `query` (required string)
- **Response Format**: text/event-stream with SSE protocol
- **Integration**: Uses OpenAI API with streaming enabled
- **Algorithm**:
  1. Parse query parameter from URL
  2. Call getDataContext to retrieve relevant inspection data
  3. Build system prompt for sewer inspection analysis
  4. Create user prompt with query + JSON context data
  5. Stream OpenAI completion using async iteration
  6. Send each chunk as SSE event: `data: {"content": "..."}\n\n`
  7. Handle completion and errors with proper SSE formatting
- **Error Handling**: Graceful fallback responses for API failures

### components/ChatInterface.tsx
- **Purpose**: Real-time chat UI with streaming messages
- **Features**:
  - Message history display (user + AI messages)
  - Streaming message rendering (word-by-word display)
  - Loading states during AI response
  - Error message handling
  - Input field with send button
  - Auto-scroll to latest messages
- **SSE Integration**: 
  - EventSource for receiving streaming data
  - Real-time message updates as chunks arrive
  - Connection management and error recovery
- **State Management**: React useState for messages and loading states

### app/chat/page.tsx
- **Purpose**: Main chat interface page
- **Layout**: Full-page chat interface with header and message area
- **Components**: Integrates ChatInterface component
- **Navigation**: Accessible via /chat route
- **Initial State**: Welcome message with example queries

## Files to Modify

### lib/types.ts
- **Current State**: Contains SewerInspection and search interfaces
- **Addition**: Add chat-specific types:
  - ChatMessage interface (role, content, timestamp)
  - AIContextData interface for structured context
  - ChatResponse interface for API responses

### app/layout.tsx
- **Purpose**: Add navigation link to chat page
- **Modification**: Include link/navigation to /chat route

### app/page.tsx
- **Purpose**: Add chat feature promotion on home page
- **Modification**: Include link to chat functionality

## Technical Requirements

### OpenAI Integration
- **Model**: GPT-3.5-turbo (as specified)
- **API Key**: Use existing OPENAI_API_KEY environment variable
- **Streaming**: Enable stream: true for real-time responses
- **Token Management**: Limit context data to stay within model limits
- **Error Handling**: Graceful degradation when API unavailable

### Server-Sent Events Implementation
- **Protocol**: Standard SSE with `text/event-stream` content type
- **Event Format**: `data: {"content": "text"}\n\n` for chunks
- **Error Events**: `data: {"error": "message"}\n\n` for failures
- **Connection Headers**: 
  - `Content-Type: text/event-stream`
  - `Cache-Control: no-cache`
  - `Connection: keep-alive`

### Context-Aware AI System
- **System Prompt**: "You are analyzing sewer inspection data. Provide specific answers based on the data."
- **User Prompt Format**: `${query}\n\nData: ${JSON.stringify(context)}`
- **Context Filtering**: Smart data selection based on query analysis
- **Response Quality**: Encourage specific numerical answers and data-driven insights

### Real-time UI Components
- **Streaming Display**: Progressive text rendering as chunks arrive
- **Message History**: Persistent chat history during session
- **Loading States**: Clear indication when AI is responding
- **Error Recovery**: Retry mechanisms for failed requests
- **Responsive Design**: Mobile-friendly chat interface

## Data Flow
1. User submits question in chat interface
2. ChatInterface sends GET request to /api/chat with query parameter
3. Chat API analyzes query and calls getDataContext function
4. Context function searches inspection data and returns relevant subset
5. API constructs OpenAI prompt with system message + user query + context data
6. OpenAI streams response chunks via async iteration
7. Each chunk sent as SSE event to frontend
8. ChatInterface receives SSE events and updates message display in real-time
9. Completed message added to chat history

## Example Query Processing
- **Query**: "What's the average inspection score in Houston?"
- **Context Selection**: Filter inspections by city="Houston"
- **AI Response**: Calculate and provide specific numerical average
- **Data Reference**: Include inspection count and score range details

## Error Handling Strategy
- **API Failures**: Return helpful error messages via SSE
- **Network Issues**: Frontend retry with exponential backoff
- **Context Overflow**: Truncate data context to fit token limits
- **Invalid Queries**: Provide guidance on supported question types
